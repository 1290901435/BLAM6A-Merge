{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74948f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T02:57:00.709425Z",
     "start_time": "2023-01-08T02:56:55.967997Z"
    }
   },
   "outputs": [],
   "source": [
    "from features import ensembleFeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from utils import metricsCal\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import Attention_model as Model\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e816f62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T02:57:02.679114Z",
     "start_time": "2023-01-08T02:57:02.666450Z"
    }
   },
   "outputs": [],
   "source": [
    "#一个函数用于序列转换，一个函数用于检测“N”碱基\n",
    "def long_short(data):\n",
    "    seq_list = []\n",
    "    for i in data[0]:\n",
    "        seq_list.append(str(i)[480:521])\n",
    "    return np.array(seq_list)\n",
    "\n",
    "def check_N(data1,data2):\n",
    "    seq_list = []\n",
    "    gene_list = []\n",
    "    for i in range(len(data1)):\n",
    "        if str(data1[i]).find(\"N\")<0:\n",
    "            seq_list.append(data1[i])\n",
    "            gene_list.append(data2[i])\n",
    "    return np.array(seq_list),np.array(gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b9c026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T02:57:09.670479Z",
     "start_time": "2023-01-08T02:57:09.655141Z"
    }
   },
   "outputs": [],
   "source": [
    "# type_name = [\"FullTranscript\",\"matureRNA\"]\n",
    "# cell_name = [\"A549\",\"CD8T\",\"Hek293_abacm\",\"Hek293_sysy\",\"HeLa\",\"MOLM13\"]\n",
    "type_name = [\"matureRNA\"]\n",
    "cell_name = [\"HeLa\",\"MOLM13\"]\n",
    "#feature_name = [\"emb\",\"PSNP\",\"PCP\",\"DBPF\"]\n",
    "feature_name = [\"PCP\"]\n",
    "network_name = [\"BILSTM+Self-Attention\",\"BILSTM+MultiSelf-Attention\",\"BILSTM+Bah-Attention\"]\n",
    "\n",
    "rows = len(type_name)*len(cell_name)*len(feature_name)*len(network_name)*5\n",
    "row = -1\n",
    "max_epochs = 50\n",
    "max_patience = 20\n",
    "batch_size = 1024\n",
    "row_name = []\n",
    "Loss = np.zeros(rows*max_epochs).reshape(rows,max_epochs)\n",
    "Acc = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "Mcc = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "Auc = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "th_All = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a4a68b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T07:56:44.629563Z",
     "start_time": "2023-01-08T02:57:19.274821Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.399439632892609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.116\n",
      "auc on test set: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.311836004257202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.176\n",
      "auc on test set: 0.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.20971167087555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.205\n",
      "auc on test set: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.073140621185303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.221\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.910976767539978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.244\n",
      "auc on test set: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.784455239772797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.640248894691467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.51987874507904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.403530895709991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.299696385860443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.213664472103119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.101212680339813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.00956529378891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.926630556583405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.852799594402313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.783258140087128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.698044538497925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.616347134113312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5323086977005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.452052116394043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.336656093597412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.242008030414581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.137091517448425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.073725581169128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.955388724803925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.850558340549469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.735\n",
      "7.733503460884094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.598346918821335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.44678071141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3664407432079315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.193096071481705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.731\n",
      "7.038719952106476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.728\n",
      "6.906209468841553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.766057312488556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.64300662279129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.723\n",
      "6.480733186006546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.290928930044174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.135838657617569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.718\n",
      "5.985185116529465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.719\n",
      "5.875086069107056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7480500638484955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.529948323965073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.709\n",
      "5.368439435958862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.218420535326004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.710\n",
      "5.019379436969757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6350220353374034 0.2724983335332116 0.6889232875790923\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.33496755361557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.154\n",
      "auc on test set: 0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.927102327346802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.233\n",
      "auc on test set: 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.576664447784424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.303293347358704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.082174003124237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.915046632289886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.74186372756958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.59031867980957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.447557032108307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.285262763500214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.13625305891037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.725\n",
      "7.971811473369598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.796964943408966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.686845153570175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.480051726102829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.720\n",
      "7.2948116064071655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.102314203977585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.714\n",
      "6.8949242532253265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.773872345685959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.611920893192291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.706\n",
      "6.499375909566879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.710\n",
      "6.257861435413361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.297\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.131352663040161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.699\n",
      "5.928893685340881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.704\n",
      "5.7459816634655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.702\n",
      "5.632180780172348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.695\n",
      "5.47011724114418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.695\n",
      "5.273188680410385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.696\n",
      "5.062896132469177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.694\n",
      "4.962952017784119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.868227988481522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.698\n",
      "4.5862859189510345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.275\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.409946441650391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.689\n",
      "0.6324950471030607 0.2650284918545562 0.6823303038583639\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.265\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.460702359676361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.067\n",
      "auc on test set: 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.334368586540222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.148\n",
      "auc on test set: 0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.219685196876526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.186\n",
      "auc on test set: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.147905230522156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.197\n",
      "auc on test set: 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.072083294391632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.203\n",
      "auc on test set: 0.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.012558341026306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.202\n",
      "auc on test set: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.987739980220795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.219\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.94720458984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.226\n",
      "auc on test set: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.918086230754852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.225\n",
      "auc on test set: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.875979244709015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.231\n",
      "auc on test set: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.848710715770721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.242\n",
      "auc on test set: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.84126192331314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.244\n",
      "auc on test set: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.81008529663086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.246\n",
      "auc on test set: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.806526243686676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.251\n",
      "auc on test set: 0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.762743592262268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.256\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.763363182544708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.260\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.748158752918243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.263\n",
      "auc on test set: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.729236781597137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.263\n",
      "auc on test set: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.726535022258759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.705914676189423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.263\n",
      "auc on test set: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.728050827980042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.270\n",
      "auc on test set: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.682117938995361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.662891387939453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.680863559246063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.660986483097076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.274\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.638635694980621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.637576460838318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.60937112569809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.275\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.619444847106934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.274\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.623659193515778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.620379149913788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.605981647968292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.273\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.593396484851837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.573068082332611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.592184007167816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.567138910293579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.568733394145966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.549375593662262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.549022793769836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.548217058181763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.537626385688782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.530498325824738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.286\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.531518876552582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.532718777656555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.684\n",
      "9.5257288813591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.297\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.501968145370483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.501810193061829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.500021755695343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.483680009841919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.472465932369232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5960457688109004 0.1921799161586965 0.6313370409771696\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.192\n",
      "auc on test set: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.18102103471756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.192\n",
      "auc on test set: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.892645359039307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.237\n",
      "auc on test set: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.650088131427765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.265\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.44439834356308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.273\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.277047753334045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.117618322372437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.969858944416046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.839741230010986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.698233246803284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.585945963859558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.422493755817413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.301441729068756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.163974404335022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.720\n",
      "8.020179331302643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.722\n",
      "7.844309538602829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.721\n",
      "7.7040349543094635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.722\n",
      "7.542275637388229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.720\n",
      "7.371271431446075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.721\n",
      "7.198214620351791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.718\n",
      "7.007236868143082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.844049900770187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.669248998165131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.715\n",
      "6.499940067529678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.709\n",
      "6.320972502231598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.13814178109169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.710\n",
      "5.917779982089996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.705\n",
      "5.69694048166275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.295\n",
      "auc on test set: 0.700\n",
      "5.532685786485672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.297\n",
      "auc on test set: 0.701\n",
      "5.335767984390259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.282\n",
      "auc on test set: 0.699\n",
      "5.169779896736145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.698\n",
      "4.955338358879089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.693\n",
      "4.759461164474487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.558826953172684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.265\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.634354910443537 0.27063785606261453 0.6866602071127199\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.271\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.339664280414581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.144\n",
      "auc on test set: 0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.183011829853058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.202\n",
      "auc on test set: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.042462646961212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.227\n",
      "auc on test set: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.930906653404236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.243\n",
      "auc on test set: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.847626388072968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.256\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.8030224442482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.71183955669403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.630659520626068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.550021767616272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.472799897193909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.434102058410645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35625284910202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.293822944164276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.251166820526123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.171530425548553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.108687341213226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.051799535751343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.006831645965576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.962683141231537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.868352591991425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.847893238067627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.76364517211914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.729815781116486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.699442088603973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.626930117607117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.625658750534058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.735\n",
      "8.572389781475067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.733\n",
      "8.51471471786499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.432870030403137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.404088139533997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.736\n",
      "8.351158261299133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.737\n",
      "8.295954883098602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.363\n",
      "auc on test set: 0.737\n",
      "8.262686908245087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.736\n",
      "8.182377457618713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.736\n",
      "8.105437457561493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.736\n",
      "8.05359673500061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.366\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.968078166246414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.369\n",
      "auc on test set: 0.737\n",
      "7.913600265979767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.366\n",
      "auc on test set: 0.737\n",
      "7.877211838960648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.761906027793884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.738\n",
      "7.708836704492569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.366\n",
      "auc on test set: 0.736\n",
      "7.637511044740677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.736\n",
      "7.55897518992424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.737\n",
      "7.4980926513671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.735\n",
      "7.432543188333511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.363\n",
      "auc on test set: 0.736\n",
      "7.382465660572052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.278349727392197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.362\n",
      "auc on test set: 0.736\n",
      "7.2092220187187195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.733\n",
      "7.133257418870926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.09494212269783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6369627623013787 0.27410224837135133 0.6873724407848751\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.274\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3676176071167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.146\n",
      "auc on test set: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.284902274608612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.174\n",
      "auc on test set: 0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.02300500869751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.218\n",
      "auc on test set: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.80704015493393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.695574402809143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.274\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.566214323043823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.48562628030777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.23368787765503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.151325762271881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.047154486179352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.976239323616028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.876328587532043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.796870768070221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.715127408504486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.609500408172607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.572665333747864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.489448726177216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.386812329292297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.338355123996735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.228595197200775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.176102221012115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.062195539474487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.02260971069336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.940786570310593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.830927073955536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.765125036239624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.558288216590881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.531216084957123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.59013494849205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.318152874708176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.216775476932526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2522198259830475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.167549431324005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.918761819601059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.940684467554092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.931489020586014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.735956490039825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.539981365203857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.570769160985947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.411790609359741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5874930918216705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.457942515611649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.190594643354416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.633020660655804 0.26690404108169696 0.6851976611525121\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.33366584777832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.165\n",
      "auc on test set: 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.941362202167511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.58359444141388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.271\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.322035908699036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.087646543979645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.309\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.86076045036316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.67029082775116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.500748753547668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.313856899738312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.107288658618927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.958425283432007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.715580642223358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.518570721149445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.30102151632309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.060446321964264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.827933132648468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.62713235616684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37246972322464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.159723550081253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.917377471923828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.720204591751099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.545706748962402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.284\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.356732189655304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.069305568933487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.286\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9100267589092255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.686402231454849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4833760261535645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.299704998731613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.151735603809357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.253\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6293009339748514 0.2586499746974165 0.6775857583403477\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.259\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.567090630531311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.137\n",
      "auc on test set: 0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.428375363349915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.195\n",
      "auc on test set: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.183858335018158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.250\n",
      "auc on test set: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.994629800319672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.264\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.932533860206604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.633993744850159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.487335979938507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.297\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.509202480316162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.24252188205719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.203804731369019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.107763290405273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.917978882789612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.776163578033447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.78835517168045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.55651080608368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.544713973999023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.498522579669952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.308709144592285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.169566333293915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.020998358726501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.865540564060211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.859120160341263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.682943016290665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4765671491622925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.466321617364883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.211309373378754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.08383584022522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0491058230400085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.705289930105209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.712751895189285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.394492030143738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.413195610046387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.166977554559708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.305\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.023227959871292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.794987827539444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6520063281059265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.512962877750397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.366563409566879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.105124771595001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.967404544353485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.286\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.777896046638489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.638195932559738 0.27701877020986804 0.6895055944341999\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.224745869636536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.206\n",
      "auc on test set: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.647709131240845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.256\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.355257868766785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.286\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.10960078239441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.926785945892334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.67290335893631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.526048302650452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.345729410648346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.093930721282959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.93528538942337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.706562578678131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.305\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.551027476787567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.291453957557678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.122223973274231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.883940100669861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.271\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.678248792886734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.446337789297104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.250077724456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.027503997087479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.270\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.906604707241058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.679315507411957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.249\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.50292843580246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.262\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3471055924892426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.059656381607056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.895233899354935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.247\n",
      "auc on test set: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.701574623584747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.629823088645935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.250\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.420343101024628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.260\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.219698071479797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.259\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6288763999514818 0.2595331191956569 0.6768067655072243\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.260\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.45406711101532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.165\n",
      "auc on test set: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.068395435810089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.225\n",
      "auc on test set: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.76500290632248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.586401522159576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.417670965194702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.349203526973724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144942939281464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.984467148780823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.918977737426758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.669944941997528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.641796112060547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.357\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.437234282493591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.212724089622498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.070224344730377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.03676462173462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.730675518512726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.582416623830795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.419812202453613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.193834662437439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.018549174070358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.797371357679367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.694421499967575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.381158530712128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.210733741521835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.295\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0067382752895355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.309\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.726057022809982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.615615516901016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3953845500946045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.329818874597549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.975494354963303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.850693225860596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.529677927494049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6294626612218494 0.26058981844173956 0.6811351917817593\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.259797513484955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.193\n",
      "auc on test set: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.965209543704987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.778426587581635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.69463312625885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.629209697246552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.564241349697113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.284\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.559484601020813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.295\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.482179760932922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.464207708835602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.423004806041718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.400639712810516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.376211166381836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.36547327041626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.34234607219696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.320715248584747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.30955696105957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.290104746818542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.294935047626495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.28365445137024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.261164784431458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.246920883655548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.250575184822083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.252506494522095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.220063030719757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.223986446857452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.225657761096954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.21030741930008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.214446306228638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.195018112659454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.191263258457184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.182110846042633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.15564650297165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.170465767383575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.173500657081604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.168003618717194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.16252726316452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.165414988994598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.139481723308563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.162727952003479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144551813602448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.143131971359253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.16392958164215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.151482582092285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.141138911247253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.357\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.154030442237854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.135613024234772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.136243402957916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.127990067005157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.134207963943481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.130520164966583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6230946508713056 0.2482054037178312 0.6720210214884315\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.333590805530548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.123\n",
      "auc on test set: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.151399195194244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.162\n",
      "auc on test set: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.048747897148132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.193\n",
      "auc on test set: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.965133965015411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.199\n",
      "auc on test set: 0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.897638082504272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.214\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.864660918712616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.227\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.817198276519775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.221\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.775125503540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.230\n",
      "auc on test set: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.74644911289215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.238\n",
      "auc on test set: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.728444635868073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.239\n",
      "auc on test set: 0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.701492309570312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.239\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.672825992107391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.246\n",
      "auc on test set: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.645274758338928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.247\n",
      "auc on test set: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.613679349422455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.251\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.604752361774445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.244\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.580326080322266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.252\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.567939460277557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.254\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.556446552276611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.249\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.542823910713196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.244\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.549857497215271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.251\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.50386095046997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.252\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.530240178108215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.253\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.50582081079483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.496476948261261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.256\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.496476233005524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.259\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.481893241405487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.262\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.466902613639832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.270\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.465385019779205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.450077056884766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.273\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.443802893161774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.268\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.44415295124054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.271\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.433054685592651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.41476857662201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.426792860031128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.282\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.422801911830902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.404914557933807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.394552946090698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.396472454071045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38508015871048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.390616476535797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.363727509975433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.366004586219788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.373664140701294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.36360502243042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.36283129453659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.295\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.364971101284027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.342593848705292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.341014564037323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.336825013160706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.308\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.343001127243042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.305\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614280515909918 0.22857425145737734 0.6567859591036728\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.229\n",
      "auc on test set: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.38748288154602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.099\n",
      "auc on test set: 0.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.302368342876434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.176\n",
      "auc on test set: 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.208751559257507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.209\n",
      "auc on test set: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.141351282596588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.208\n",
      "auc on test set: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.07015061378479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.221\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.032378017902374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.230\n",
      "auc on test set: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.966155886650085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.226\n",
      "auc on test set: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.93821406364441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.229\n",
      "auc on test set: 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.904403448104858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.231\n",
      "auc on test set: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.861763596534729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.232\n",
      "auc on test set: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.826158583164215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.242\n",
      "auc on test set: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.789840400218964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.755921483039856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.252\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.744771957397461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.252\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.717605412006378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.252\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.703900039196014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.700462281703949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.259\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.680906057357788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.642221748828888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.269\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.618306577205658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.259\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.622017204761505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.269\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.593111395835876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.266\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.56556099653244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.263\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.565627992153168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.268\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.520683586597443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.503032386302948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.502146780490875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.484504222869873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.282\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.469205021858215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.45448487997055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.453681111335754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.41828191280365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.417976021766663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.284\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.40338808298111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.455475747585297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.394753217697144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.397619724273682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.358613431453705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.282\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.369408369064331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38834273815155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.335275650024414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.335935056209564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.336552143096924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.33614993095398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.28913962841034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.286031663417816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.292784929275513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.301188051700592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.276522696018219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.301130950450897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6126834593458133 0.23207836898556278 0.6617826227067929\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.232\n",
      "auc on test set: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.400369584560394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.088\n",
      "auc on test set: 0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.358059048652649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.138\n",
      "auc on test set: 0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3308846950531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.167\n",
      "auc on test set: 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.295465052127838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.186\n",
      "auc on test set: 0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.254219353199005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.189\n",
      "auc on test set: 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.214069664478302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.191\n",
      "auc on test set: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.191943883895874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.192\n",
      "auc on test set: 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.147054314613342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.203\n",
      "auc on test set: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.09637314081192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.199\n",
      "auc on test set: 0.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.047725915908813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.201\n",
      "auc on test set: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.997359573841095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.210\n",
      "auc on test set: 0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.969723522663116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.213\n",
      "auc on test set: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.93266624212265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.217\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.896548867225647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.220\n",
      "auc on test set: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.845694720745087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.238\n",
      "auc on test set: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.80814528465271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.240\n",
      "auc on test set: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.784774243831635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.243\n",
      "auc on test set: 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.757007956504822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.246\n",
      "auc on test set: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.727707147598267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.246\n",
      "auc on test set: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.688372910022736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.240\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.687775313854218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.65692538022995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.245\n",
      "auc on test set: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.634762644767761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.245\n",
      "auc on test set: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.60538387298584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.596425473690033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.580352425575256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.253\n",
      "auc on test set: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.561618566513062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.253\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.55144053697586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.256\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.52970266342163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.251\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.510691821575165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.506490409374237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.482346057891846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.472031712532043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.258\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.454049825668335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.45028829574585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.438424587249756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.263\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.438598453998566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.267\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.407444298267365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.275\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.399252355098724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.393637478351593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.383800148963928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.384747564792633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.364831984043121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.283\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35708874464035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35577666759491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.271\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.352262735366821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.342851996421814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.32819402217865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.334442853927612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.325936615467072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6074879715360045 0.21609696545864665 0.6484650961689843\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.216\n",
      "auc on test set: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.378388524055481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.110\n",
      "auc on test set: 0.581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.326575219631195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.140\n",
      "auc on test set: 0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.283601343631744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.168\n",
      "auc on test set: 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.23583596944809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.174\n",
      "auc on test set: 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.199970304965973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.177\n",
      "auc on test set: 0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.159625470638275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.192\n",
      "auc on test set: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.116025149822235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.203\n",
      "auc on test set: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.079096019268036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.205\n",
      "auc on test set: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.032517671585083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.216\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99813985824585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.210\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.96655660867691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.222\n",
      "auc on test set: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.933829307556152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.206\n",
      "auc on test set: 0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.902824878692627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.232\n",
      "auc on test set: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.87728875875473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.231\n",
      "auc on test set: 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.843660354614258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.235\n",
      "auc on test set: 0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.82012414932251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.247\n",
      "auc on test set: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.784721493721008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.249\n",
      "auc on test set: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.759128391742706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.260\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.741443634033203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.728510797023773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.685219645500183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.253\n",
      "auc on test set: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.66182541847229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.268\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.651372015476227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.266\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.638292610645294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.612729132175446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.603806853294373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.605463862419128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.568143784999847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.282\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.564194560050964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.538683891296387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.532134473323822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.504990816116333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.49999451637268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.488854050636292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.309\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.464806318283081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.464443504810333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.449767351150513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.438541233539581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.419134497642517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.411631107330322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.396924793720245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.395922899246216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.370699107646942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.373479068279266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.352372825145721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.342788636684418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.335455298423767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.335849225521088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.333307445049286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.308649897575378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6110055391582097 0.22282496125897505 0.65157879522483\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.223\n",
      "auc on test set: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.654979646205902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.100\n",
      "auc on test set: 0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5320006608963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.157\n",
      "auc on test set: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.43912935256958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.170\n",
      "auc on test set: 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.31268286705017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.181\n",
      "auc on test set: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.20233464241028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.188\n",
      "auc on test set: 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.054459512233734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.199\n",
      "auc on test set: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.913091778755188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.229\n",
      "auc on test set: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.751865327358246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.586658895015717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.271\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.439771115779877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.30146723985672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.20512968301773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.081052601337433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.969333827495575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.858107149600983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.766335129737854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.660099864006042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.593265175819397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.510567665100098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.419023752212524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.345330536365509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.270836770534515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.218760192394257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.12496691942215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.043944776058197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.976964354515076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.890992939472198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.359\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.821288049221039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.74619060754776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.679641842842102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.561263144016266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.476729452610016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.368\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.39411586523056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.363\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.291343092918396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.359\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.198087632656097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.119022011756897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.368\n",
      "auc on test set: 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.990001916885376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.88657832145691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.367\n",
      "auc on test set: 0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.753656029701233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.61978429555893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.363\n",
      "auc on test set: 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.478205412626266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.365\n",
      "auc on test set: 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.346403926610947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.365\n",
      "auc on test set: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.186035126447678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.05196350812912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.362\n",
      "auc on test set: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.893786638975143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.360\n",
      "auc on test set: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.700776427984238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.362\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.563336938619614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.345\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.377838730812073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.204184144735336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.992590188980103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6112845673505798 0.22263252996463959 0.6540520025210065\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.223\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.53386300802231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.167\n",
      "auc on test set: 0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.32864785194397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.189\n",
      "auc on test set: 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.17148780822754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.216\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.833140790462494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.42818546295166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.001239657402039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.635330200195312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.38079434633255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.164538025856018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.990107834339142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.799760341644287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.643163025379181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.357\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.480891108512878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.306662261486053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.146388351917267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.916696667671204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.736201822757721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.49333906173706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.29798310995102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.054486393928528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.81051680445671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.606445014476776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.354079723358154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.120331823825836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.86296859383583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.616497486829758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.300915509462357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.081188559532166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.828212141990662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.529868453741074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.232656687498093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.953781396150589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.297\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.692379921674728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.37454628944397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.138357430696487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.841407746076584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5623486042022705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2315744906663895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.045104384422302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.818074524402618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6165811775200714 0.23316555719183021 0.6539272217065177\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.233\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.645250499248505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.159\n",
      "auc on test set: 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.2880277633667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.196\n",
      "auc on test set: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.137079060077667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.212\n",
      "auc on test set: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.050291121006012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.223\n",
      "auc on test set: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.96483290195465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.208\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.886565327644348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.221\n",
      "auc on test set: 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.822175741195679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.229\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.73489761352539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.239\n",
      "auc on test set: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.666198253631592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.250\n",
      "auc on test set: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.597186088562012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.260\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.577408373355865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.266\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.493318438529968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.274\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.478936493396759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.407011449337006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.312667191028595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.271140813827515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.167710483074188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.125011682510376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.078457593917847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.958562433719635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.92948031425476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.867070019245148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.764312863349915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.635821282863617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.58345764875412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.47130662202835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.394446432590485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.328610837459564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.259808957576752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.180372178554535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.134673833847046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.069357335567474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.998139381408691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.887121438980103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.819196462631226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.800534844398499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.626049935817719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.532416939735413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.480017244815826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.359\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.371805846691132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.265795767307281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.15878438949585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.115022480487823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.362\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.98905885219574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.83959025144577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.362\n",
      "auc on test set: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.765259325504303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.368\n",
      "auc on test set: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.607355892658234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.573036402463913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.398818761110306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.250483244657516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6113403211418377 0.2240865635919517 0.6511295290381262\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.224\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.28826093673706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.223\n",
      "auc on test set: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.448224306106567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.86244136095047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.432536900043488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.169877171516418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.907197952270508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.667597949504852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.360\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4420245885849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.203594863414764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.910998165607452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.672118425369263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.434897482395172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.213045746088028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.942415803670883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.345\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.679876863956451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.441207498311996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.185275346040726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.937894731760025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.70680770277977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.422566503286362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.210495173931122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.308\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.857552111148834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.612199008464813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.465943455696106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.177363008260727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.861523985862732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.619822174310684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.409681856632233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6108385370205174 0.2220169032476554 0.6518129170841156\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.222\n",
      "auc on test set: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.379918456077576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.156\n",
      "auc on test set: 0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.992559552192688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.194\n",
      "auc on test set: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.771402537822723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.214\n",
      "auc on test set: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.616850137710571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.236\n",
      "auc on test set: 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.49770736694336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.244\n",
      "auc on test set: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4077188372612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.250\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.312496066093445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.265\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.236299574375153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.269\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.164173126220703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.113839268684387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.278\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.058410406112671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.010354101657867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.941186785697937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.305\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.904685020446777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.308\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.841708660125732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.804818212985992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.741771399974823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.702146887779236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.305\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.647232949733734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.602473556995392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.56634134054184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.507630467414856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.461447417736053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.413687288761139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.363662719726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.3138747215271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.269906520843506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.212092161178589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.156909108161926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.101095080375671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.056645452976227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.01306414604187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.937146067619324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.867905974388123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.806263327598572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.76244968175888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.702359020709991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.345\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.611224293708801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.55434936285019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.449101090431213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.39652156829834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.290830671787262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.230391502380371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.139639854431152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.048634052276611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.964761853218079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.898540437221527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.779850482940674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.358\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.665286779403687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.573825418949127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6140722569134701 0.22822900467901572 0.653599071353712\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.228\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.164934277534485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.200\n",
      "auc on test set: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.850257933139801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.376805782318115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.91671472787857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.62946331501007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.288833975791931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.08638072013855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.9146089553833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.581011891365051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.357530534267426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.126545190811157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.922126591205597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.76607146859169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.572036772966385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.36923149228096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.937855809926987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.801798790693283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.462929964065552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.377713352441788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.004335314035416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.63987022638321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.350364416837692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.994418919086456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.898260325193405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.59232810139656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.36022213101387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.104727774858475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.698893517255783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.335713416337967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.145276963710785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.849853307008743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6075490633363069 0.21510824909823248 0.6498000609859936\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.215\n",
      "auc on test set: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.62732970714569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.172\n",
      "auc on test set: 0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.053081035614014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.221\n",
      "auc on test set: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.68479061126709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.417137920856476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.123368918895721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.850802183151245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.559320271015167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.367289900779724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.211120843887329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.022347569465637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.891697764396667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.357\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.714787423610687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.59963470697403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.360\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4095858335495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.231186330318451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.08250081539154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.359\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.946306765079498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.361\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.747111797332764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.364\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.563926070928574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.40034481883049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.251211047172546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.977531045675278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.790014296770096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.64793136715889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.345\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.420644164085388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.167685598134995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.930940181016922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.345\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.741987824440002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.608246624469757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.326495170593262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.154380053281784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.947231024503708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.678214758634567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.450462102890015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.299122869968414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05992603302002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.870743304491043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.610749691724777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.481344431638718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618978590544157 0.23802881506415352 0.6594827811304104\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.238\n",
      "auc on test set: 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.48838108778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.230\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.732576668262482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.277\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.203474879264832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.710565865039825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.375175476074219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.144253492355347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.877289056777954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.648157775402069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.357\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.384569466114044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.182599306106567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.92652553319931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.66004228591919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.356\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.44265130162239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.144158512353897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.863734304904938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.547843307256699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.346\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3389131128788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.040043026208878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.730699628591537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.525878816843033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.139565467834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.864268273115158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.553485333919525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.295725673437119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.072073489427567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.641532629728317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.294466763734818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.079188644886017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.808570206165314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.500325173139572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2675789296627045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6116190900981266 0.2233181641960684 0.6533299573525772\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.223\n",
      "auc on test set: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.873404264450073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.170\n",
      "auc on test set: 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.49026471376419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.264\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.00305998325348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.507927536964417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.057727456092834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.834480702877045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.337\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.560281872749329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.134727239608765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.360\n",
      "auc on test set: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.022397100925446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.790626108646393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.363\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.57992798089981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.409018099308014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.123573362827301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.811087191104889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.752839237451553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.362\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.34291186928749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.353\n",
      "auc on test set: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.966005384922028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.982975274324417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.851210683584213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.651922523975372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.295958310365677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.210988223552704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.509438574314117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.715494334697723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.33432587981224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.022294580936432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.420209407806396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.304622858762741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.217580139636993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.304\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.93630826473236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.754695922136307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6148528099910794 0.2299077818071993 0.6554382688154373\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.230\n",
      "auc on test set: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.221423864364624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.195\n",
      "auc on test set: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.609020113945007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.254\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.068234205245972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.655139863491058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.313226640224457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.038849413394928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.834797441959381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.57005125284195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.339925944805145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.088353395462036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.869123220443726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.614970564842224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.323723047971725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.085278391838074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.88986074924469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.588874369859695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.329513520002365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.073189407587051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.734047889709473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.394862473011017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.133669227361679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.797633349895477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.474992901086807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.301\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.309563547372818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.295\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.043801039457321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.698669373989105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.300\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.424804329872131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.04911157488823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.868626117706299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.597046136856079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.183533579111099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6117863514719001 0.22454345653496635 0.6522600789028051\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.225\n",
      "auc on test set: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.36349046230316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.194\n",
      "auc on test set: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.873796582221985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.253\n",
      "auc on test set: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.618713200092316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.265\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.447324812412262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.280\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.319133579730988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.223228633403778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.118594646453857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.306\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.047252178192139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.000176906585693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.316\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.925171077251434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.317\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.880787253379822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.84091705083847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.805083870887756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.778560280799866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.751721441745758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.728544354438782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.730889201164246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.696054220199585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.672589421272278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.661717712879181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.344\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.637695610523224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.343\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.635081112384796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.345\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.607026994228363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.352\n",
      "auc on test set: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.622311651706696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.584931075572968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.574875235557556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.348\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.580281794071198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.564421117305756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.559354484081268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.554190635681152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.495640993118286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.349\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.465959429740906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.467462480068207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.350\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.46826547384262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.44975858926773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.351\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.455710053443909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.355\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.442668855190277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.357\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.452740371227264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.428946673870087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.347\n",
      "auc on test set: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.413604259490967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.360\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.408725261688232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.419991075992584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.354\n",
      "auc on test set: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.401837885379791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.359\n",
      "auc on test set: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6035347903657449 0.20730913437361956 0.6423227116475588\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.207\n",
      "auc on test set: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.670840203762054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.118\n",
      "auc on test set: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.35571312904358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.150\n",
      "auc on test set: 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.184415340423584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.180\n",
      "auc on test set: 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.052758157253265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.193\n",
      "auc on test set: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.96399986743927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.195\n",
      "auc on test set: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.887710452079773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.201\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.826772391796112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.209\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.777295589447021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.215\n",
      "auc on test set: 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.732508480548859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.214\n",
      "auc on test set: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.685986459255219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.214\n",
      "auc on test set: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.641511917114258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.223\n",
      "auc on test set: 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.609317600727081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.228\n",
      "auc on test set: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.56533694267273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.234\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.531001329421997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.239\n",
      "auc on test set: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.49581229686737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.235\n",
      "auc on test set: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.460556983947754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.243\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.429896771907806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.395093739032745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.371168494224548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.261\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.339067876338959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.263\n",
      "auc on test set: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.310490489006042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.264\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.289566278457642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.269\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.25893259048462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.274\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.225728809833527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.205053627490997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.282\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.187749743461609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.161976337432861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.132363557815552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.10542768239975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.090530097484589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.068713903427124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.048008024692535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.01363867521286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.992433965206146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.293\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.973891615867615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.298\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.953414738178253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.940866887569427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.897819578647614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.885471642017365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.866230607032776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.308\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.855112552642822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.821209490299225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.804368436336517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.79508751630783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.778490900993347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.311\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.765661418437958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.749364674091339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.728260159492493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.726346850395203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.698741972446442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975691347011597 0.19528366746905418 0.633926419468647\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.195\n",
      "auc on test set: 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.603610634803772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.141\n",
      "auc on test set: 0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.392832279205322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.195\n",
      "auc on test set: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.23427927494049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.208\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.118023812770844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.217\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.99697494506836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.226\n",
      "auc on test set: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.901483595371246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.226\n",
      "auc on test set: 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8167325258255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.235\n",
      "auc on test set: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.755797386169434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.254\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.679503679275513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.250\n",
      "auc on test set: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.626755595207214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.254\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.579499244689941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.260\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.535392820835114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.264\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.481987118721008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.272\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.434977769851685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.270\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.399923384189606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.285\n",
      "auc on test set: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.327986061573029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.291\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.289331912994385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.253463327884674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.225836455821991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.292\n",
      "auc on test set: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.202766478061676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.299\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.154740869998932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.110934019088745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.090759873390198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.308\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.056291699409485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.033603310585022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.00465726852417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.313\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.9897301197052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.982294499874115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.940715312957764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.922476828098297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.902281880378723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.318\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.878402590751648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.864909708499908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.853030264377594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.830946207046509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.321\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.80896657705307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.322\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.805410146713257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.791353464126587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.786083221435547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.77860027551651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.750682532787323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.770300686359406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.334\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.745303213596344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.745749771595001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.737697958946228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.723016619682312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.69564574956894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.691532492637634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.686668813228607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.333\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.683685421943665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5945584299732382 0.18970898236927525 0.629589491947183\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.190\n",
      "auc on test set: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.396434009075165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.153\n",
      "auc on test set: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.004207968711853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.211\n",
      "auc on test set: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.7464200258255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.252\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.570218980312347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.258\n",
      "auc on test set: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.458030581474304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.270\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.372965574264526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.276\n",
      "auc on test set: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.292686760425568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.286\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.250155746936798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.288\n",
      "auc on test set: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.18664687871933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.295\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.157421588897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.117929637432098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.08472204208374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.303\n",
      "auc on test set: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.046876430511475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.302\n",
      "auc on test set: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.015861928462982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.307\n",
      "auc on test set: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.991340160369873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.315\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.984500169754028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.309\n",
      "auc on test set: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.951943695545197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.312\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.941344559192657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.310\n",
      "auc on test set: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.909788310527802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.901886463165283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.319\n",
      "auc on test set: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.881910681724548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.86214965581894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.85532546043396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.833887040615082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.325\n",
      "auc on test set: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.818581342697144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.326\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.819851160049438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.789202749729156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.773998439311981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.323\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.757826507091522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.760066390037537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.330\n",
      "auc on test set: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.746942162513733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.324\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.75387442111969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.335\n",
      "auc on test set: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.7443687915802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.328\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.71219527721405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.705650806427002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.329\n",
      "auc on test set: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.685753405094147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.68424278497696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.674322605133057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.327\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.679409086704254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.658150732517242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.331\n",
      "auc on test set: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.657938122749329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.332\n",
      "auc on test set: 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.648436903953552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.650816202163696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.338\n",
      "auc on test set: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.639898002147675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.645358443260193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.339\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.64295107126236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.621459007263184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.595093429088593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.341\n",
      "auc on test set: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.60115647315979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.340\n",
      "auc on test set: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.590108573436737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.342\n",
      "auc on test set: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6008586083853702 0.2035138428301039 0.6410541139533885\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.204\n",
      "auc on test set: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.672819912433624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.097\n",
      "auc on test set: 0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.491341412067413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.116\n",
      "auc on test set: 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.353221774101257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.129\n",
      "auc on test set: 0.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.277916371822357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.133\n",
      "auc on test set: 0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.200112521648407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.152\n",
      "auc on test set: 0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.137228965759277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.164\n",
      "auc on test set: 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.082301139831543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.162\n",
      "auc on test set: 0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.029910445213318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.164\n",
      "auc on test set: 0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.979293048381805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.176\n",
      "auc on test set: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.955111622810364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.180\n",
      "auc on test set: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.924125075340271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.180\n",
      "auc on test set: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.868628084659576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.187\n",
      "auc on test set: 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.820097506046295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.194\n",
      "auc on test set: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.782626032829285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.198\n",
      "auc on test set: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.75325632095337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.198\n",
      "auc on test set: 0.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.703161656856537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.201\n",
      "auc on test set: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.66949725151062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.199\n",
      "auc on test set: 0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.65499085187912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.212\n",
      "auc on test set: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.61366468667984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.213\n",
      "auc on test set: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.58855950832367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.215\n",
      "auc on test set: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.56190949678421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.216\n",
      "auc on test set: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.522570192813873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.225\n",
      "auc on test set: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.488398909568787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.224\n",
      "auc on test set: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.468118369579315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.226\n",
      "auc on test set: 0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.448446154594421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.227\n",
      "auc on test set: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.450340747833252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.232\n",
      "auc on test set: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.396891713142395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.230\n",
      "auc on test set: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.36719137430191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.251\n",
      "auc on test set: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.354446828365326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.249\n",
      "auc on test set: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.33879280090332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.238\n",
      "auc on test set: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.313438951969147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.242\n",
      "auc on test set: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.30696839094162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.248\n",
      "auc on test set: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.269717872142792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.255\n",
      "auc on test set: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.259564220905304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.257\n",
      "auc on test set: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.267206132411957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.259\n",
      "auc on test set: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.217885255813599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.264\n",
      "auc on test set: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.222949147224426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.270\n",
      "auc on test set: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.187573432922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.275\n",
      "auc on test set: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.174638867378235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.273\n",
      "auc on test set: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.179739534854889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.279\n",
      "auc on test set: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.140845477581024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.275\n",
      "auc on test set: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.12199091911316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.275\n",
      "auc on test set: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.111975729465485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.284\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.103262305259705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.287\n",
      "auc on test set: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.08182281255722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.284\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.053320288658142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.296\n",
      "auc on test set: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.048848152160645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.286\n",
      "auc on test set: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.027475535869598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.281\n",
      "auc on test set: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.008665263652802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.294\n",
      "auc on test set: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.990453541278839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.289\n",
      "auc on test set: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5895405887600357 0.17985687078857826 0.6243590676454135\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.180\n",
      "auc on test set: 0.624\n"
     ]
    }
   ],
   "source": [
    "Folder_name = []\n",
    "for typei in range(len(type_name)):\n",
    "    for celli in range(len(cell_name)):\n",
    "        Folder_list = []\n",
    "        trainPosSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_seq.csv\"\n",
    "        trainNegSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_seq.csv\"\n",
    "        testPosSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_seq.csv\"\n",
    "        testNegSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_seq.csv\"\n",
    "        trainPosGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_gene.csv\"\n",
    "        trainNegGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_gene.csv\"\n",
    "        testPosGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_gene.csv\"\n",
    "        testNegGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_gene.csv\"\n",
    "        \n",
    "        #获取文件信息\n",
    "        trainPos_seq = pd.read_csv(trainPosSeq_file,header=None)\n",
    "        trainNeg_seq = pd.read_csv(trainNegSeq_file,header=None)\n",
    "        testPos_seq = pd.read_csv(testPosSeq_file,header=None)\n",
    "        testNeg_seq = pd.read_csv(testNegSeq_file,header=None)\n",
    "        trainPos_gene = pd.read_csv(trainPosGene_file) \n",
    "        trainNeg_gene = pd.read_csv(trainNegGene_file)\n",
    "        testPos_gene = pd.read_csv(testPosGene_file)\n",
    "        testNeg_gene = pd.read_csv(testNegGene_file)\n",
    "        #序列转41-nt长度\n",
    "        trainPos_seq = long_short(trainPos_seq)\n",
    "        trainNeg_seq = long_short(trainNeg_seq)\n",
    "        testPos_seq = long_short(testPos_seq)\n",
    "        testNeg_seq = long_short(testNeg_seq)\n",
    "        #基因转35长度\n",
    "        trainPos_gene = trainPos_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        trainNeg_gene = trainNeg_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        testPos_gene = testPos_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        testNeg_gene = testNeg_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        trainPos_seq,trainPos_gene = check_N(trainPos_seq,np.array(trainPos_gene))\n",
    "        trainNeg_seq,trainNeg_gene = check_N(trainNeg_seq,np.array(trainNeg_gene))\n",
    "        testPos_seq,testPos_gene = check_N(testPos_seq,np.array(testPos_gene))\n",
    "        testNeg_seq,testNeg_gene = check_N(testNeg_seq,np.array(testNeg_gene))\n",
    "        #基因变Float\n",
    "        trainPos_gene = np.float32(trainPos_gene)\n",
    "        trainNeg_gene = np.float32(trainNeg_gene)\n",
    "        testPos_gene = np.float32(testPos_gene)\n",
    "        testNeg_gene = np.float32(testNeg_gene)\n",
    "        #打乱顺序，再取等长\n",
    "        trainNeg_seq, trainNeg_gene = shuffle(trainNeg_seq,trainNeg_gene, random_state=1)\n",
    "        testNeg_seq, testNeg_gene = shuffle(testNeg_seq, testNeg_gene, random_state=1)\n",
    "\n",
    "        trainNeg_seq = trainNeg_seq[:len(trainPos_seq)]\n",
    "        trainNeg_gene = trainNeg_gene[:len(trainPos_gene)]\n",
    "        testNeg_seq = testNeg_seq[:len(testPos_seq)]\n",
    "        testNeg_gene = testNeg_gene[:len(testPos_gene)]\n",
    "        \n",
    "#         trainNeg_seq = trainNeg_seq[:len(trainPos_seq)*10]\n",
    "#         trainNeg_gene = trainNeg_gene[:len(trainPos_gene)*10]\n",
    "#         testNeg_seq = testNeg_seq[:len(testPos_seq)*10]\n",
    "#         testNeg_gene = testNeg_gene[:len(testPos_gene)*10]\n",
    "#         trainPos_seq = trainPos_seq.repeat(10)\n",
    "#         trainPos_gene = trainPos_gene.repeat(10)\n",
    "#         testPos_seq = testPos_seq.repeat(10)\n",
    "#         testPos_gene = testPos_gene.repeat(10)\n",
    "\n",
    "        #第二重循环\n",
    "        for featurei in range(len(feature_name)):\n",
    "            if feature_name[featurei] == \"emb\":\n",
    "                trainPos = ensembleFeature.emb_seqs(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.emb_seqs(trainNeg_seq)\n",
    "                testPos = ensembleFeature.emb_seqs(testPos_seq)\n",
    "                testNeg = ensembleFeature.emb_seqs(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"PSNP\":\n",
    "                trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSNP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq)\n",
    "            elif feature_name[featurei] == \"PCP\":\n",
    "                trainPos = ensembleFeature.PCP(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.PCP(trainNeg_seq)\n",
    "                testPos = ensembleFeature.PCP(testPos_seq)\n",
    "                testNeg = ensembleFeature.PCP(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"DBPF\":\n",
    "                trainPos = ensembleFeature.DBPF(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.DBPF(trainNeg_seq)\n",
    "                testPos = ensembleFeature.DBPF(testPos_seq)\n",
    "                testNeg = ensembleFeature.DBPF(testNeg_seq)\n",
    "            if len(testNeg.shape)==2:\n",
    "                trainPos = trainPos.reshape(trainPos.shape[0],1,trainPos.shape[1])\n",
    "                trainNeg = trainNeg.reshape(trainNeg.shape[0],1,trainNeg.shape[1])\n",
    "                testPos = testPos.reshape(testPos.shape[0],1,testPos.shape[1])\n",
    "                testNeg = testNeg.reshape(testNeg.shape[0],1,testNeg.shape[1])\n",
    "#                 trainPos = trainPos.reshape(trainPos.shape[0],trainPos.shape[1],1)\n",
    "#                 trainNeg = trainNeg.reshape(trainNeg.shape[0],trainNeg.shape[1],1)\n",
    "#                 testPos = testPos.reshape(testPos.shape[0],testPos.shape[1],1)\n",
    "#                 testNeg = testNeg.reshape(testNeg.shape[0],testNeg.shape[1],1)\n",
    "\n",
    "            trainPos_label = np.ones(len(trainPos))\n",
    "            trainNeg_label = np.zeros(len(trainNeg))\n",
    "            testPos_label = np.ones(len(testPos))\n",
    "            testNeg_label = np.zeros(len(testNeg))\n",
    "\n",
    "            trainData = np.append(trainPos,trainNeg,axis=0)\n",
    "            testData = np.append(testPos,testNeg,axis = 0)\n",
    "            trainLabel = np.append(trainPos_label,trainNeg_label,axis = 0)\n",
    "            testLabel = np.append(testPos_label,testNeg_label,axis = 0)\n",
    "\n",
    "            kf = KFold(5,True,10)\n",
    "            #第三重循环\n",
    "            for networki in range(len(network_name)):\n",
    "                if network_name[networki] == \"BILSTM+Self-Attention\":\n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "#                         if i != 0:\n",
    "#                             continue\n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                        best_auc = 0\n",
    "                        patience = 0\n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                        model = Model.ModelBS_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "                        \n",
    "                        for j in range(max_epochs):\n",
    "                            runningLoss,th = Model.train(model,X_train,Y_train,i,device,model_dir,batch_size)\n",
    "                            acc, mcc, auc=Model.test(X_test,Y_test,best_auc,device,model_dir,batch_size,th)\n",
    "                            Loss[row][j] = runningLoss\n",
    "                            th_All[row][j] = th\n",
    "                            Acc[row][j] = acc\n",
    "                            Mcc[row][j] = mcc\n",
    "                            Auc[row][j] = auc\n",
    "                           \n",
    "                            if auc > best_auc:\n",
    "                                best_auc=auc\n",
    "                                patience=0\n",
    "                            else:\n",
    "                                patience+=1\n",
    "                            if patience>max_patience and best_auc!= 0.5:\n",
    "                                break\n",
    "                        tmp_th = 0.5\n",
    "                        tmp_auc = 0\n",
    "                        for kk in range(max_epochs):\n",
    "                            if Auc[row][kk]>tmp_auc:\n",
    "                                tmp_th = th_All[row][kk]\n",
    "                                tmp_auc = Auc[row][kk]\n",
    "                        acc, mcc, auc=Model.independTest(testData,testLabel,device,model_dir,batch_size,tmp_th)\n",
    "                        Acc[row][-1] = acc\n",
    "                        Mcc[row][-1] = mcc\n",
    "                        Auc[row][-1] = auc\n",
    "                        th_All[row][-1] = tmp_th\n",
    "\n",
    "                if network_name[networki] == \"BILSTM+MultiSelf-Attention\":\n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "                        \n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                        best_auc = 0\n",
    "                        patience = 0\n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                        model = Model.ModelB_MultiSelf_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "                      \n",
    "                        for j in range(max_epochs):\n",
    "                            runningLoss,th = Model.train(model,X_train,Y_train,i,device,model_dir,batch_size)\n",
    "                            acc, mcc, auc=Model.test(X_test,Y_test,best_auc,device,model_dir,batch_size,th)\n",
    "                            Loss[row][j] = runningLoss\n",
    "                            th_All[row][j] = th\n",
    "                            Acc[row][j] = acc\n",
    "                            Mcc[row][j] = mcc\n",
    "                            Auc[row][j] = auc\n",
    "                            \n",
    "                            if auc > best_auc:\n",
    "                                best_auc=auc\n",
    "                                patience=0\n",
    "                            else:\n",
    "                                patience+=1\n",
    "                            if patience>max_patience and best_auc!= 0.5:\n",
    "                                break\n",
    "                        tmp_th = 0.5\n",
    "                        tmp_auc = 0\n",
    "                        for kk in range(max_epochs):\n",
    "                            if Auc[row][kk]>tmp_auc:\n",
    "                                tmp_th = th_All[row][kk]\n",
    "                                tmp_auc = Auc[row][kk]\n",
    "                        acc, mcc, auc=Model.independTest(testData,testLabel,device,model_dir,batch_size,tmp_th)\n",
    "                        Acc[row][-1] = acc\n",
    "                        Mcc[row][-1] = mcc\n",
    "                        Auc[row][-1] = auc\n",
    "                        th_All[row][-1] = tmp_th\n",
    "                \n",
    "                if network_name[networki] == \"BILSTM+Bah-Attention\":\n",
    "\n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "#                         if i != 0:\n",
    "#                             continue\n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                        best_auc = 0\n",
    "                        patience = 0\n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                        model = Model.ModelB_Bah_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "                        \n",
    "                        for j in range(max_epochs):\n",
    "                            runningLoss,th = Model.train(model,X_train,Y_train,i,device,model_dir,batch_size)\n",
    "                            acc, mcc, auc=Model.test(X_test,Y_test,best_auc,device,model_dir,batch_size,th)\n",
    "                            Loss[row][j] = runningLoss\n",
    "                            th_All[row][j] = th\n",
    "                            Acc[row][j] = acc\n",
    "                            Mcc[row][j] = mcc\n",
    "                            Auc[row][j] = auc\n",
    "                            \n",
    "                            if auc > best_auc:\n",
    "                                best_auc=auc\n",
    "                                patience=0\n",
    "                            else:\n",
    "                                patience+=1\n",
    "                            if patience>max_patience and best_auc!= 0.5:\n",
    "                                 break\n",
    "                        tmp_th = 0.5\n",
    "                        tmp_auc = 0\n",
    "                        for kk in range(max_epochs):\n",
    "                            if Auc[row][kk]>tmp_auc:\n",
    "                                tmp_th = th_All[row][kk]\n",
    "                                tmp_auc = Auc[row][kk]\n",
    "                        acc, mcc, auc=Model.independTest(testData,testLabel,device,model_dir,batch_size,tmp_th)\n",
    "                        Acc[row][-1] = acc\n",
    "                        Mcc[row][-1] = mcc\n",
    "                        Auc[row][-1] = auc\n",
    "                        th_All[row][-1] = tmp_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81152a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T02:50:41.086181Z",
     "start_time": "2023-01-08T02:50:40.893291Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Mcc,axis=1)).to_csv(\"Attention_Result/Mcc_PCP.csv\",index=0)\n",
    "# pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Auc,axis=1)).to_csv(\"Attention_Result/Auc_PCP.csv\",index=0)\n",
    "# pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Acc,axis=1)).to_csv(\"Attention_Result/Acc_PCP.csv\",index=0)\n",
    "# pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),th_All,axis=1)).to_csv(\"Attention_Result/th_PCP.csv\",index=0)\n",
    "# pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Loss,axis=1)).to_csv(\"Attention_Result/Loss_PCP.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd662a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T09:59:35.320612Z",
     "start_time": "2023-01-08T09:59:34.994913Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Mcc,axis=1)).to_csv(\"Attention_Result/Mcc_PCP_append.csv\",index=0)\n",
    "pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Auc,axis=1)).to_csv(\"Attention_Result/Auc_PCP_append.csv\",index=0)\n",
    "pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Acc,axis=1)).to_csv(\"Attention_Result/Acc_PCP_append.csv\",index=0)\n",
    "pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),th_All,axis=1)).to_csv(\"Attention_Result/th_PCP_append.csv\",index=0)\n",
    "pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Loss,axis=1)).to_csv(\"Attention_Result/Loss_PCP_append.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205dca29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T05:52:52.629169Z",
     "start_time": "2023-02-21T05:52:52.592846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6779548746213933, 0.6771204286397188, 0.6138322100508202)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Auc = np.array(pd.read_csv(\"Attention_Result/Auc_PCP.csv\").iloc[:,1:])\n",
    "print(Auc.shape)\n",
    "\n",
    "x1 = 0\n",
    "x2 = 0\n",
    "x3 = 0\n",
    "for i in range(3):\n",
    "\n",
    "    for j in range(12):\n",
    "        if i == 0:\n",
    "            x1+=np.mean(Auc[15*j+5*i:15*j+5*i+5,-1])\n",
    "        if i == 1:\n",
    "            x2+=np.mean(Auc[15*j+5*i:15*j+5*i+5,-1])\n",
    "        if i == 2:\n",
    "            x3+=np.mean(Auc[15*j+5*i:15*j+5*i+5,-1])\n",
    "x1/12,x2/12,x3/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"BILSTM+Self-Attention\",\"BILSTM+MultiHead-Attention\",\"BILSTM+Bah-Attention\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BERT]",
   "language": "python",
   "name": "conda-env-BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
