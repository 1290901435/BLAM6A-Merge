{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3030973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T10:23:13.898705Z",
     "start_time": "2023-03-07T10:23:11.862846Z"
    }
   },
   "outputs": [],
   "source": [
    "from features import ensembleFeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from utils import metricsCal\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import Attention_model as Model\n",
    "#import Model\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import csv\n",
    "from itertools import combinations,permutations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "from thundersvm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a80687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T10:23:14.011221Z",
     "start_time": "2023-03-07T10:23:14.005551Z"
    }
   },
   "outputs": [],
   "source": [
    "#一个函数用于序列转换，一个函数用于检测“N”碱基\n",
    "def long_short(data):\n",
    "    seq_list = []\n",
    "    for i in data[0]:\n",
    "        seq_list.append(str(i)[480:521])\n",
    "    return np.array(seq_list)\n",
    "\n",
    "def check_N(data1,data2):\n",
    "    seq_list = []\n",
    "    gene_list = []\n",
    "    for i in range(len(data1)):\n",
    "        if str(data1[i]).find(\"N\")<0:\n",
    "            seq_list.append(data1[i])\n",
    "            gene_list.append(data2[i])\n",
    "    return np.array(seq_list),np.array(gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cddc76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T10:23:14.124217Z",
     "start_time": "2023-03-07T10:23:14.118288Z"
    }
   },
   "outputs": [],
   "source": [
    "type_name = [\"FullTranscript\",\"matureRNA\"]\n",
    "cell_name = [\"A549\",\"CD8T\",\"Hek293_abacm\",\"Hek293_sysy\",\"HeLa\",\"MOLM13\"]\n",
    "# type_name = [\"matureRNA\"]\n",
    "# cell_name = [\"MOLM13\"]\n",
    "#feature_name = [\"DBPF\",\"emb\",\"PCP\",\"PSNP\"]\n",
    "feature_name=[\"PCP\"]\n",
    "network_name = [\"BILSTM+Self-Attention\",\"BILSTM+MultiSelf-Attention\",\"BILSTM+Bah-Attention\"]\n",
    "#network_name = [\"BILSTM+Self-Attention\",\"BILSTM+MultiSelf-Attention\",\"BILSTM+Bah-Attention\",\"BILSTM+MultiHead-Attention\"]\n",
    "\n",
    "#rows = len(type_name)*len(cell_name)*len(feature_name)*len(network_name)*5\n",
    "rows = len(type_name)*len(cell_name)*len(feature_name)*len(network_name)*5\n",
    "row = -1\n",
    "batch_size = 400\n",
    "row_name = []\n",
    "parameter_header_record = [\"name\",\"th\",\"TN\",\"FN\",\"FP\",\"TP\",\"Sen\",\"Spe\",\"Acc\",\"Mcc\",\"AUC\"]\n",
    "\n",
    "Parameter = np.zeros(rows*10).reshape(rows,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c787a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:45:56.181779Z",
     "start_time": "2023-03-07T10:23:14.658427Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_A549_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_CD8T_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_HeLa_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTranscript_MOLM13_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_A549_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_CD8T_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_abacm_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_Hek293_sysy_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_HeLa_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Self-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Self-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Self-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Self-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Self-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.add(out,torch.tensor(out_one))\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn = torch.add(attn,torch.tensor(attn_one))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+MultiSelf-Attention_KFold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Bah-Attention_KFold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Bah-Attention_KFold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Bah-Attention_KFold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Bah-Attention_KFold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
      "/data/xiayunpeng/PycharmWorkspace/Whistle/Attention_model.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matureRNA_MOLM13_PCP_BILSTM+Bah-Attention_KFold_4\n"
     ]
    }
   ],
   "source": [
    "Folder_name = []\n",
    "for typei in range(len(type_name)):\n",
    "    for celli in range(len(cell_name)):\n",
    "        Folder_list = []\n",
    "        trainPosSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_seq.csv\"\n",
    "        trainNegSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_seq.csv\"\n",
    "        testPosSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_seq.csv\"\n",
    "        testNegSeq_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_seq.csv\"\n",
    "        trainPosGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_gene.csv\"\n",
    "        trainNegGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_train_\"+type_name[typei]+\"_gene.csv\"\n",
    "        testPosGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Pos_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_gene.csv\"\n",
    "        testNegGene_file = \"data/\"+type_name[typei]+\"/\"+cell_name[celli]+\"/Neg_\"+cell_name[celli]+\"_test_\"+type_name[typei]+\"_gene.csv\"\n",
    "        \n",
    "        #获取文件信息\n",
    "        trainPos_seq = pd.read_csv(trainPosSeq_file,header=None)\n",
    "        trainNeg_seq = pd.read_csv(trainNegSeq_file,header=None)\n",
    "        testPos_seq = pd.read_csv(testPosSeq_file,header=None)\n",
    "        testNeg_seq = pd.read_csv(testNegSeq_file,header=None)\n",
    "        trainPos_gene = pd.read_csv(trainPosGene_file) \n",
    "        trainNeg_gene = pd.read_csv(trainNegGene_file)\n",
    "        testPos_gene = pd.read_csv(testPosGene_file)\n",
    "        testNeg_gene = pd.read_csv(testNegGene_file)\n",
    "        #序列转41-nt长度\n",
    "        trainPos_seq = long_short(trainPos_seq)\n",
    "        trainNeg_seq = long_short(trainNeg_seq)\n",
    "        testPos_seq = long_short(testPos_seq)\n",
    "        testNeg_seq = long_short(testNeg_seq)\n",
    "        #基因转35长度\n",
    "        trainPos_gene = trainPos_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        trainNeg_gene = trainNeg_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        testPos_gene = testPos_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        testNeg_gene = testNeg_gene.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,13,14,16,17,19,23,22,25,20,21,44,45,46,47,48,49,50,65,66,69,55,51,57,56]]\n",
    "        trainPos_seq,trainPos_gene = check_N(trainPos_seq,np.array(trainPos_gene))\n",
    "        trainNeg_seq,trainNeg_gene = check_N(trainNeg_seq,np.array(trainNeg_gene))\n",
    "        testPos_seq,testPos_gene = check_N(testPos_seq,np.array(testPos_gene))\n",
    "        testNeg_seq,testNeg_gene = check_N(testNeg_seq,np.array(testNeg_gene))\n",
    "        #基因变Float\n",
    "        trainPos_gene = np.float32(trainPos_gene)\n",
    "        trainNeg_gene = np.float32(trainNeg_gene)\n",
    "        testPos_gene = np.float32(testPos_gene)\n",
    "        testNeg_gene = np.float32(testNeg_gene)\n",
    "        #打乱顺序，再取等长\n",
    "        trainNeg_seq, trainNeg_gene = shuffle(trainNeg_seq,trainNeg_gene, random_state=1)\n",
    "        testNeg_seq, testNeg_gene = shuffle(testNeg_seq, testNeg_gene, random_state=1)\n",
    "\n",
    "        trainNeg_seq = trainNeg_seq[:len(trainPos_seq)]\n",
    "        trainNeg_gene = trainNeg_gene[:len(trainPos_gene)]\n",
    "        testNeg_seq = testNeg_seq[:len(testPos_seq)]\n",
    "        testNeg_gene = testNeg_gene[:len(testPos_gene)]\n",
    "        \n",
    "        #建立空表\n",
    "        header_name = [\"true\"]\n",
    "        \n",
    "        #第二重循环\n",
    "        for featurei in range(len(feature_name)):\n",
    "            if feature_name[featurei] == \"binary\":\n",
    "                trainPos = ensembleFeature.Binary(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.Binary(trainNeg_seq)\n",
    "                testPos = ensembleFeature.Binary(testPos_seq)\n",
    "                testNeg = ensembleFeature.Binary(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"DNC\":\n",
    "                trainPos = ensembleFeature.DNC(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.DNC(trainNeg_seq)\n",
    "                testPos = ensembleFeature.DNC(testPos_seq)\n",
    "                testNeg = ensembleFeature.DNC(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"NCPA\":\n",
    "                trainPos = ensembleFeature.NCPA(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.NCPA(trainNeg_seq)\n",
    "                testPos = ensembleFeature.NCPA(testPos_seq)\n",
    "                testNeg = ensembleFeature.NCPA(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"gene\":\n",
    "                trainPos = trainPos_gene\n",
    "                trainNeg = trainNeg_gene\n",
    "                testPos = testPos_gene\n",
    "                testNeg = testNeg_gene\n",
    "            elif feature_name[featurei] == \"emb\":\n",
    "                trainPos = ensembleFeature.emb_seqs(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.emb_seqs(trainNeg_seq)\n",
    "                testPos = ensembleFeature.emb_seqs(testPos_seq)\n",
    "                testNeg = ensembleFeature.emb_seqs(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"PSNP\":\n",
    "                trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSNP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq)\n",
    "            elif feature_name[featurei] == \"ENAC\":\n",
    "                trainPos = ensembleFeature.ENAC(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.ENAC(trainNeg_seq)\n",
    "                testPos = ensembleFeature.ENAC(testPos_seq)\n",
    "                testNeg = ensembleFeature.ENAC(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"EIIP\":\n",
    "                trainPos = ensembleFeature.EIIP(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.EIIP(trainNeg_seq)\n",
    "                testPos = ensembleFeature.EIIP(testPos_seq)\n",
    "                testNeg = ensembleFeature.EIIP(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"PseDNC\":\n",
    "                trainPos = ensembleFeature.PseDNC(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.PseDNC(trainNeg_seq)\n",
    "                testPos = ensembleFeature.PseDNC(testPos_seq)\n",
    "                testNeg = ensembleFeature.PseDNC(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"PCP\":\n",
    "                trainPos = ensembleFeature.PCP(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.PCP(trainNeg_seq)\n",
    "                testPos = ensembleFeature.PCP(testPos_seq)\n",
    "                testNeg = ensembleFeature.PCP(testNeg_seq)\n",
    "            elif feature_name[featurei] == \"DBPF\":\n",
    "                trainPos = ensembleFeature.DBPF(trainPos_seq)\n",
    "                trainNeg = ensembleFeature.DBPF(trainNeg_seq)\n",
    "                testPos = ensembleFeature.DBPF(testPos_seq)\n",
    "                testNeg = ensembleFeature.DBPF(testNeg_seq)\n",
    "            if len(testNeg.shape)==2:\n",
    "                trainPos = trainPos.reshape(trainPos.shape[0],1,trainPos.shape[1])\n",
    "                trainNeg = trainNeg.reshape(trainNeg.shape[0],1,trainNeg.shape[1])\n",
    "                testPos = testPos.reshape(testPos.shape[0],1,testPos.shape[1])\n",
    "                testNeg = testNeg.reshape(testNeg.shape[0],1,testNeg.shape[1])\n",
    "\n",
    "            trainPos_label = np.ones(len(trainPos))\n",
    "            trainNeg_label = np.zeros(len(trainNeg))\n",
    "            testPos_label = np.ones(len(testPos))\n",
    "            testNeg_label = np.zeros(len(testNeg))\n",
    "\n",
    "            trainData = np.append(trainPos,trainNeg,axis=0)\n",
    "            testData = np.append(testPos,testNeg,axis = 0)\n",
    "            trainLabel = np.append(trainPos_label,trainNeg_label,axis = 0)\n",
    "            testLabel = np.append(testPos_label,testNeg_label,axis = 0)\n",
    "            \n",
    "            if featurei == 0:\n",
    "                result_table = testLabel.reshape(-1,1)\n",
    "\n",
    "            kf = KFold(5,True,10)    \n",
    "            #第三重循环\n",
    "            for networki in range(len(network_name)):\n",
    "                if network_name[networki] == \"BILSTM+Self-Attention\":         \n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "                        header_name.append(row_one)\n",
    "\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                \n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                    \n",
    "                        model = Model.ModelBS_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        \n",
    "                        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "                        \n",
    "                        \n",
    "                        y_pred,y_true = Model.independResult(X_train,Y_train,device,model_dir,batch_size,0.5)\n",
    "                        Parameter[row][0],_,_,_,_,_,_,_,_,_ = metricsCal.get_train_metrics(y_pred,y_true)\n",
    "                        y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,Parameter[row][0])\n",
    "                        Parameter[row][1],Parameter[row][2],Parameter[row][3],Parameter[row][4],Parameter[row][5],Parameter[row][6],Parameter[row][7],Parameter[row][8],Parameter[row][9] = metricsCal.get_test_metrics(y_pred,y_true,Parameter[row][0])\n",
    "                        #y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,0.5) \n",
    "                        result_table = np.append(result_table,y_pred.reshape(-1,1),axis = 1)\n",
    "                        print(row_one)\n",
    "                \n",
    "                if network_name[networki] == \"BILSTM+MultiSelf-Attention\":         \n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "                        header_name.append(row_one)\n",
    "\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                \n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                    \n",
    "                        model = Model.ModelB_MultiSelf_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        \n",
    "                        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "                        \n",
    "                        y_pred,y_true = Model.independResult(X_train,Y_train,device,model_dir,batch_size,0.5)\n",
    "                        Parameter[row][0],_,_,_,_,_,_,_,_,_ = metricsCal.get_train_metrics(y_pred,y_true)\n",
    "                        y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,Parameter[row][0])\n",
    "                        Parameter[row][1],Parameter[row][2],Parameter[row][3],Parameter[row][4],Parameter[row][5],Parameter[row][6],Parameter[row][7],Parameter[row][8],Parameter[row][9] = metricsCal.get_test_metrics(y_pred,y_true,Parameter[row][0])\n",
    "                        #y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,0.5) \n",
    "                        result_table = np.append(result_table,y_pred.reshape(-1,1),axis = 1)\n",
    "                        print(row_one)\n",
    "                        \n",
    "                if network_name[networki] == \"BILSTM+Bah-Attention\":         \n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "                        header_name.append(row_one)\n",
    "\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                \n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                    \n",
    "                        model = Model.ModelB_Bah_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        \n",
    "                        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "                        \n",
    "                        y_pred,y_true = Model.independResult(X_train,Y_train,device,model_dir,batch_size,0.5)\n",
    "                        Parameter[row][0],_,_,_,_,_,_,_,_,_ = metricsCal.get_train_metrics(y_pred,y_true)\n",
    "                        y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,Parameter[row][0])\n",
    "                        Parameter[row][1],Parameter[row][2],Parameter[row][3],Parameter[row][4],Parameter[row][5],Parameter[row][6],Parameter[row][7],Parameter[row][8],Parameter[row][9] = metricsCal.get_test_metrics(y_pred,y_true,Parameter[row][0])\n",
    "                        #y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,0.5) \n",
    "                        result_table = np.append(result_table,y_pred.reshape(-1,1),axis = 1)\n",
    "                        print(row_one)\n",
    "                if network_name[networki] == \"BILSTM+MultiHead-Attention\":         \n",
    "                    for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "                        row+=1\n",
    "                        row_one = type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\"_\"+network_name[networki]+\"_KFold_\" + str(i)\n",
    "                        row_name.append(row_one)\n",
    "                        header_name.append(row_one)\n",
    "\n",
    "                        X_train = trainData[train_index]\n",
    "                        X_test = trainData[test_index]\n",
    "                        Y_train = trainLabel[train_index]\n",
    "                        Y_test = trainLabel[test_index]\n",
    "                \n",
    "                        model_dir = \"Model_Attention_Extraction_Final/\"+type_name[typei]+\"_\"+cell_name[celli]+\"/\"+\\\n",
    "                                feature_name[featurei]+\"_\"+network_name[networki]+\"/KFold_\" + str(i) + \"/\"\n",
    "                    \n",
    "                        model = Model.ModelB_Multi_Pro(trainData.shape[1],trainData.shape[2])\n",
    "                        \n",
    "                        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "                        os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "                        \n",
    "                        y_pred,y_true = Model.independResult(X_train,Y_train,device,model_dir,batch_size,0.5)\n",
    "                        Parameter[row][0],_,_,_,_,_,_,_,_,_ = metricsCal.get_train_metrics(y_pred,y_true)\n",
    "                        y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,Parameter[row][0])\n",
    "                        Parameter[row][1],Parameter[row][2],Parameter[row][3],Parameter[row][4],Parameter[row][5],Parameter[row][6],Parameter[row][7],Parameter[row][8],Parameter[row][9] = metricsCal.get_test_metrics(y_pred,y_true,Parameter[row][0])\n",
    "                        #y_pred,y_true = Model.independResult(testData,testLabel,device,model_dir,batch_size,0.5) \n",
    "                        result_table = np.append(result_table,y_pred.reshape(-1,1),axis = 1)\n",
    "                        print(row_one)\n",
    "        pd.DataFrame(result_table).to_csv(\"First_Second_Result/\"+type_name[typei]+\"_\"+cell_name[celli]+\"_\"+feature_name[featurei]+\".csv\",index=0,header=header_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077b0821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T02:05:09.106105Z",
     "start_time": "2023-03-08T02:05:09.076871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.53407288e-01, 4.87100000e+03, 3.26500000e+03, 4.09700000e+03,\n",
       "       5.70300000e+03, 6.35927743e-01, 5.43153434e-01, 5.89540589e-01,\n",
       "       1.79856871e-01, 6.24359068e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c3b282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T02:05:36.203089Z",
     "start_time": "2023-03-08T02:05:36.036020Z"
    }
   },
   "outputs": [],
   "source": [
    "header_record = [\"name\",\"th\",\"TN\",\"FN\",\"FP\",\"TP\",\"Sen\",\"Spe\",\"Acc\",\"Mcc\",\"AUC\"]\n",
    "pd.DataFrame(np.append(np.array(row_name).reshape(len(row_name),1),Parameter,axis=1)).to_csv(\"First_Second_Result/PCP_Parameter.csv\",index=0,header=header_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1997169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BERT]",
   "language": "python",
   "name": "conda-env-BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
